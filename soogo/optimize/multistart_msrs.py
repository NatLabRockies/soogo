"""Multistart LMSRS optimization routine."""

# Copyright (c) 2025 Alliance for Energy Innovation, LLC
# Copyright (C) 2014 Cornell University

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from copy import deepcopy
from typing import Callable, Optional
import logging
import warnings

import numpy as np

from ..acquisition import CoordinatePerturbation, BoundedParameter
from ..model import Surrogate
from .utils import OptimizeResult
from ..termination import RobustCondition, UnsuccessfulImprovement
from .surrogate_optimization import surrogate_optimization

logger = logging.getLogger(__name__)


def multistart_msrs(
    fun,
    bounds,
    maxeval: int,
    *,
    surrogateModel: Optional[Surrogate] = None,
    batchSize: int = 1,
    disp: bool = False,
    callback: Optional[Callable[[OptimizeResult], None]] = None,
    seed=None,
) -> OptimizeResult:
    """Minimize a scalar function of one or more variables using a response
    surface model approach with restarts.

    This implementation generalizes the algorithms Multistart LMSRS from [#]_.
    The general algorithm calls :func:`.surrogate_optimization()` successive
    times until there are no more function evaluations available. The first
    time :func:`.surrogate_optimization()` is called with the given, if
    any, trained surrogate model. Other function calls use an empty
    surrogate model. This is done to enable truly different starting samples
    each time.

    :param fun: The objective function to be minimized.
    :param bounds: List with the limits [x_min,x_max] of each direction x in the
        search space.
    :param maxeval: Maximum number of function evaluations.
    :param surrogateModel: Surrogate model to be used. Only used as input, not
        updated. If None is provided, :func:`.surrogate_optimization()` will
        choose a default model.
    :param batchSize: Number of new sample points to be generated per iteration.
    :param disp: Deprecated and ignored. Configure logging instead using
        standard Python logging levels.
    :param callback: If provided, the callback function will be called after
        each iteration with the current optimization result.
    :param seed: Seed or random number generator.
    :return: The optimization result.

    References
    ----------
    .. [#] Rommel G Regis and Christine A Shoemaker. A stochastic radial basis
        function method for the global optimization of expensive functions.
        INFORMS Journal on Computing, 19(4):497â€“509, 2007.
    """
    if disp:
        warnings.warn(
            "'disp' is deprecated and ignored; use logging levels instead",
            DeprecationWarning,
            stacklevel=2,
        )

    dim = len(bounds)  # Dimension of the problem
    if dim <= 0:
        raise ValueError("bounds must define at least one dimension")

    # Initialize output
    out = OptimizeResult()
    out.x = np.full(dim, np.nan)
    out.fx = np.inf
    out.sample = np.zeros((maxeval, dim))
    out.fsample = np.zeros(maxeval)

    # Copy the surrogate model
    _surrogateModel = deepcopy(surrogateModel)

    # Create random number generator
    rng = np.random.default_rng(seed)

    # do until max number of f-evals reached
    while out.nfev < maxeval:
        # Acquisition function
        acquisitionFunc = CoordinatePerturbation(
            perturbation_strategy="fixed",
            pool_size=min(1000 * dim, 10000),
            weightpattern=(0.95,),
            termination=RobustCondition(
                UnsuccessfulImprovement(), max(5, dim)
            ),
            sigma=BoundedParameter(0.1, 0.1 * 0.5**5, 0.1),
            seed=rng.integers(np.iinfo(np.int32).max).item(),
        )
        acquisitionFunc.success_period = maxeval  # to never increase sigma

        # Run local optimization
        out_local = surrogate_optimization(
            fun,
            bounds,
            maxeval - out.nfev,
            surrogateModel=_surrogateModel,
            acquisitionFunc=acquisitionFunc,
            batchSize=batchSize,
            callback=callback,
            seed=rng.integers(np.iinfo(np.int32).max).item(),
        )
        assert isinstance(out_local.fx, float), (
            "Expected float, got %s" % type(out_local.fx)
        )

        # Update output
        if out_local.fx < out.fx:
            out.x[:] = out_local.x
            out.fx = out_local.fx
        out.sample[out.nfev : out.nfev + out_local.nfev, :] = out_local.sample
        out.fsample[out.nfev : out.nfev + out_local.nfev] = out_local.fsample
        out.nfev = out.nfev + out_local.nfev

        # Update counters
        out.nit = out.nit + 1

        # Reset the surrogate model
        if _surrogateModel is not None:
            _surrogateModel.reset_data()

    return out
